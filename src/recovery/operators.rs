//! Timely operators to implement recovery.
//!
//! They do not implement user-facing dataflow logic.
//!
//! To implement a _recoverable_ operator, see
//! [`crate::operators::stateful_unary`].

use std::collections::hash_map::DefaultHasher;
use std::hash::Hash;
use std::hash::Hasher;

use timely::dataflow::channels::pact::Exchange;
use timely::dataflow::channels::pact::Pipeline;
use timely::dataflow::operators::flow_controlled::iterator_source;
use timely::dataflow::operators::flow_controlled::IteratorSourceInput;
use timely::dataflow::operators::generic::builder_rc::OperatorBuilder;
use timely::dataflow::operators::*;
use timely::dataflow::ProbeHandle;
use timely::dataflow::Scope;
use timely::dataflow::Stream;
use timely::order::TotalOrder;
use timely::progress::Timestamp;
use timely::Data;
use timely::ExchangeData;

use crate::timely::CapabilityVecEx;
use crate::timely::EagerNotificator;
use crate::timely::FrontierEx;
use crate::timely::InBuffer;
use crate::worker::WorkerCount;
use crate::worker::WorkerIndex;

use super::model::*;
use super::store::in_mem::*;

/// All stateful operators take this keyed item as input and emit this
/// as output.
///
/// We need a routeable key to ensure that all items go to the same
/// worker to see the same state.
pub(crate) type StatefulStream<S, V> = Stream<S, (StateKey, V)>;

/// Timely's name for a dataflow stream where you only care about the
/// progress messages.
pub(crate) type ClockStream<S> = Stream<S, Tick>;

/// A stream of changes to some store that can be applied by writing
/// or generated by reading.
type KChangeStream<S, K, V> = Stream<S, KChange<K, V>>;

/// An ordered stream of all changes happening in all stateful operators in a
/// dataflow.
pub(crate) type FlowChangeStream<S> = Stream<S, FlowChange>;

/// An ordered stream of changes to the state store.
pub(crate) type StoreChangeStream<S> = Stream<S, StoreChange>;

/// An ordered stream of changes to the summary the GC operator needs.
pub(crate) type StoreSummaryStream<S> = Stream<S, StoreChangeSummary>;

/// A stream of changes to a progress store.
pub(crate) type ProgressStream<S> = Stream<S, ProgressChange>;

pub(crate) trait Route {
    /// Hash this key for Timely.
    ///
    /// Timely uses the result here to decide which worker to send
    /// this data.
    fn route(&self) -> u64;

    /// Determine if this key would route to this worker.
    fn is_local(&self, index: WorkerIndex, count: WorkerCount) -> bool {
        // My read of
        // https://github.com/TimelyDataflow/timely-dataflow/blob/v0.12.0/timely/src/dataflow/channels/pushers/exchange.rs#L61-L90
        (self.route() % count.0 as u64) as usize == index.0
    }
}

impl Route for WorkerKey {
    /// As this is the key into the progress store during backup, this
    /// can be evenly distributed.
    fn route(&self) -> u64 {
        let mut hasher = DefaultHasher::new();
        self.hash(&mut hasher);
        hasher.finish()
    }
}

impl Route for StateKey {
    fn route(&self) -> u64 {
        let mut hasher = DefaultHasher::new();
        self.hash(&mut hasher);
        hasher.finish()
    }
}

impl Route for StoreKey {
    /// As this is the key into the progress store during backup, this
    /// can be evenly distributed.
    fn route(&self) -> u64 {
        let mut hasher = DefaultHasher::new();
        self.hash(&mut hasher);
        hasher.finish()
    }
}

impl Route for FlowKey {
    /// This is hacky, but whenever we're writing by FlowKey, it's
    /// actually for recovering operator state which is routed by
    /// StateKey, so we have to really route there.
    fn route(&self) -> u64 {
        let FlowKey(_step_id, state_key) = self;
        state_key.route()
    }
}

pub(crate) trait Backup<S>
where
    S: Scope,
{
    /// Turn dataflow state changes into an epoch-stamped changelog of
    /// writes to the state store.
    ///
    /// The reverse of this is [`Recover::recover`].
    fn backup(&self) -> StoreChangeStream<S>;
}

impl<S> Backup<S> for FlowChangeStream<S>
where
    S: Scope<Timestamp = u64>,
{
    fn backup(&self) -> StoreChangeStream<S> {
        let mut tmp_incoming = Vec::new();

        // This is effectively "map with epoch", but Timely doesn't
        // give us that.
        self.unary(Pipeline, "backup", |_init_cap, _info| {
            move |input, output| {
                input.for_each(|cap, incoming| {
                    let epoch = SnapshotEpoch(*cap.time());

                    assert!(tmp_incoming.is_empty());
                    incoming.swap(&mut tmp_incoming);

                    let mut session = output.session(&cap);
                    session.give_iterator(tmp_incoming.drain(..).map(
                        |KChange(flow_key, step_change)| {
                            KChange(StoreKey(epoch, flow_key), Change::Upsert(step_change))
                        },
                    ));
                })
            }
        })
    }
}

pub(crate) trait Progress<S, D>
where
    S: Scope,
    D: Data,
{
    /// Write out the current frontier at this point.
    ///
    /// The write happens just before the frontier advances, and thus
    /// is actually within the previous epoch.
    ///
    /// Doesn't emit the "empty frontier" (even though that is the
    /// true frontier) on dataflow termination to allow dataflow
    /// continuation.
    fn progress(&self, resume_epoch: ResumeEpoch, worker_key: WorkerKey) -> ProgressStream<S>;
}

impl<S, D> Progress<S, D> for Stream<S, D>
where
    S: Scope<Timestamp = u64>,
    D: Data,
{
    fn progress(&self, resume_epoch: ResumeEpoch, worker_key: WorkerKey) -> ProgressStream<S> {
        // We can't use a notificator for progress because it's
        // possible a worker, due to partitioning, will have no input
        // data. We still need to write out that the worker made it
        // through the resume epoch in that case.
        let mut op_builder = OperatorBuilder::new("progress".to_string(), self.scope());

        let mut input = op_builder.new_input(self, Pipeline);

        let (mut output_wrapper, output_stream) = op_builder.new_output();

        let info = op_builder.operator_info();
        let activator = self.scope().activator_for(&info.address[..]);

        // Sort of "emit at end of epoch" but Timely doesn't give us
        // that.
        op_builder.build(move |mut init_caps| {
            // Since we might emit downstream without any incoming
            // items, like reporting progress on EOF, ensure we FFWD
            // to the resume epoch.
            init_caps.downgrade_all(&resume_epoch.0);
            let mut cap = init_caps.pop();

            let mut tmp_incoming = Vec::new();

            move |input_frontiers| {
                input.for_each(|_cap, incoming| {
                    assert!(tmp_incoming.is_empty());
                    incoming.swap(&mut tmp_incoming);
                    // We have to drain the incoming data, but we just
                    // care about the epoch so drop it.
                    tmp_incoming.clear();
                });

                cap = cap.take().and_then(|cap| {
                    let frontier = input_frontiers.simplify();
                    // EOF counts as progress. This will also filter
                    // out the flash of 0 epoch upon resume.
                    let frontier_progressed = frontier.map_or(true, |f| f > *cap.time());
                    if frontier_progressed {
                        // There's no way to guarantee that "last
                        // frontier + 1" is actually the resume epoch
                        // on the next execution, but mark that this
                        // worker is ready to resume there on
                        // EOF. It's also possible that this results
                        // in a "too small" resume epoch: if for some
                        // reason this operator isn't activated during
                        // every epoch, we might miss the "largest"
                        // epoch and so we'll mark down the resume
                        // epoch as one too small. That's fine, we
                        // just might resume further back than is
                        // optimal.
                        let write_frontier = frontier.unwrap_or(*cap.time() + 1);
                        let msg = ProgressMsg::Advance(WorkerFrontier(write_frontier));
                        let kchange = KChange(worker_key, Change::Upsert(msg));
                        // Do not delay cap before the write; we will
                        // delay downstream progress messages longer than
                        // necessary if so. This would manifest as
                        // resuming from an epoch that seems "too
                        // early". Write out the progress at the end of
                        // each epoch and where the frontier has moved.
                        output_wrapper.activate().session(&cap).give(kchange);

                        // If EOF, drop caps after the write.
                        frontier.map(|f| {
                            // We should never delay to something like
                            // frontier + 1, otherwise chained progress
                            // operators will "drift" forward and GC will
                            // happen too early. If the frontier is empty,
                            // also drop the capability.
                            cap.delayed(&f)
                        })
                    // If there was no frontier progress on this
                    // awake, maintain the current cap and do nothing.
                    } else {
                        Some(cap)
                    }
                });

                // Wake up constantly, because we never know when
                // frontier might have advanced.
                if cap.is_some() {
                    activator.activate();
                }
            }
        });

        output_stream
    }
}

pub(crate) trait Write<S, K, V, W>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    K: ExchangeData + Route,
    V: ExchangeData,
    W: KWriter<K, V> + 'static,
{
    /// Write a stream of keyed changes to a matching store in order.
    ///
    /// [`read`] should return an equivalent version of the written
    /// stream.
    fn write(&self, writer: W) -> ClockStream<S>;
}

impl<S, K, V, W> Write<S, K, V, W> for KChangeStream<S, K, V>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    K: ExchangeData + Route,
    V: ExchangeData,
    W: KWriter<K, V> + 'static,
{
    fn write(&self, mut writer: W) -> ClockStream<S> {
        // This is effectively "aggregate" but with `KChange`s not `(K,
        // V)`. It's just slightly longer, but clearer to write it out
        // ourselves.
        self.unary_frontier(
            Exchange::new(|KChange(key, _change): &KChange<K, V>| key.route()),
            "write",
            move |init_cap, _info| {
                let mut inbuf = InBuffer::new();
                let mut ncater = EagerNotificator::new(vec![init_cap], ());

                move |input, output| {
                    input.for_each(|cap, incoming| {
                        let epoch = cap.time();
                        inbuf.extend(epoch.clone(), incoming);
                        ncater.notify_at(epoch.clone());
                    });

                    // Use notificator so writes are in epoch order per
                    // key.
                    ncater.for_each(
                        &[input.frontier().clone()],
                        |caps, ()| {
                            let cap = &caps[0];
                            let epoch = cap.time();

                            if let Some(incoming) = inbuf.remove(epoch) {
                                writer.write_many(incoming);
                            }
                        },
                        |caps, ()| {
                            let cap = &caps[0];

                            let mut session = output.session(&cap);
                            session.give(());
                        },
                        |()| {},
                    );
                }
            },
        )
    }
}

pub(crate) trait BroadcastWrite<S, K, V, W>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    K: ExchangeData,
    V: ExchangeData,
    W: KWriter<K, V> + 'static,
{
    /// This is identical to [`Write::write`] but because of how
    /// exchange pacts work, we have to manually insert broadcast.
    fn broadcast_write(&self, writer: W) -> ClockStream<S>;
}

impl<S, K, V, W> BroadcastWrite<S, K, V, W> for KChangeStream<S, K, V>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    K: ExchangeData,
    V: ExchangeData,
    W: KWriter<K, V> + 'static,
{
    fn broadcast_write(&self, mut writer: W) -> ClockStream<S> {
        self.broadcast()
            .unary_frontier(Pipeline, "write", move |init_cap, _info| {
                let mut inbuf = InBuffer::new();
                let mut ncater = EagerNotificator::new(vec![init_cap], ());

                move |input, output| {
                    input.for_each(|cap, incoming| {
                        let epoch = cap.time();
                        inbuf.extend(epoch.clone(), incoming);
                        ncater.notify_at(epoch.clone());
                    });

                    ncater.for_each(
                        &[input.frontier().clone()],
                        |caps, ()| {
                            let cap = &caps[0];
                            let epoch = cap.time();

                            if let Some(incoming) = inbuf.remove(epoch) {
                                writer.write_many(incoming);
                            }
                        },
                        |caps, ()| {
                            let cap = &caps[0];

                            let mut session = output.session(cap);
                            session.give(());
                        },
                        |()| {},
                    );
                }
            })
    }
}

/// Read a stream of keyed changes from a matching store in order.
///
/// [`Write::write`] should be used to write that stream.
pub(crate) fn read<S, K, V, R>(
    scope: &S,
    mut reader: R,
    probe: &ProbeHandle<S::Timestamp>,
) -> KChangeStream<S, K, V>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    K: Data,
    V: Data,
    R: KReader<K, V> + 'static,
{
    iterator_source(
        scope,
        "read",
        move |last_cap| {
            // Assumes that changes are read in order they are
            // written.
            reader.read_many().map(|changes| IteratorSourceInput {
                lower_bound: S::Timestamp::minimum(),
                // An iterator of (timestamp, iterator of
                // items). Nested [`IntoIterator`]s.
                data: Some((S::Timestamp::minimum(), changes)),
                target: last_cap.clone(),
            })
        },
        probe.clone(),
    )
}

pub(crate) trait Recover<S>
where
    S: Scope,
{
    /// Collapse a stream of state store changes into a stream of
    /// dataflow state changes.
    ///
    /// This effectively reverses [`Backup::backup`] by discarding any
    /// state information that's not from the latest epoch.
    ///
    /// If you don't want to collapse to the latest state, filter out
    /// the state store changes by epoch.
    fn recover(&self) -> FlowChangeStream<S>;
}

impl<S> Recover<S> for StoreChangeStream<S>
where
    S: Scope<Timestamp = u64>,
{
    fn recover(&self) -> FlowChangeStream<S> {
        let state = InMemStore::new();

        self.unary_frontier(
            Exchange::new(|KChange(StoreKey(_epoch, flow_key), _change)| flow_key.route()),
            "recover",
            move |init_cap, _info| {
                let mut inbuf = InBuffer::new();
                let mut ncater = EagerNotificator::new(vec![init_cap], state);

                move |input, output| {
                    input.for_each(|cap, incoming| {
                        let epoch = cap.time();
                        inbuf.extend(*epoch, incoming);
                        ncater.notify_at(*epoch);
                    });

                    ncater.for_each(
                        &[input.frontier().clone()],
                        |caps, state| {
                            let cap = &caps[0];
                            let epoch = cap.time();

                            if let Some(changes) = inbuf.remove(epoch) {
                                state.write_many(changes);
                                // Remove all but the newest changes so we don't
                                // have to have the whole recovery store in mem.
                                state.filter_last();
                            }
                        },
                        |caps, state| {
                            let cap = &caps[0];

                            let mut session = output.session(&cap);
                            session.give_iterator(state.drain_flatten().into_iter());
                        },
                        |_state| {},
                    );
                }
            },
        )
    }
}

pub(crate) trait Summary<S>
where
    S: Scope,
{
    /// Drop the data contained within state store changes.
    fn summary(&self) -> StoreSummaryStream<S>;
}

impl<S> Summary<S> for StoreChangeStream<S>
where
    S: Scope,
{
    fn summary(&self) -> StoreSummaryStream<S> {
        self.map(|KChange(store_key, change)| KChange(store_key, change.map(|c| c.typ())))
    }
}

pub(crate) trait GarbageCollect<S>
where
    S: Scope,
{
    /// Generate deletes for un-needed state store data.
    ///
    /// This looks at the stream of worker progress events, maintains
    /// a snapshot of the current cluster frontier, and emits delete
    /// changes when historical data is no longer necessary for
    /// recovery.
    ///
    /// Note, GCing a write to the state store is not the same as that
    /// state being discarded by user logic: the fact that state was
    /// discarded is itself written to the state change log; GC is
    /// when we no longer need that part of the change log because the
    /// work for that epoch is finalized.
    ///
    /// This takes a summary of writes to the state store in order to
    /// not actually need to hold the state in memory.
    fn garbage_collect(
        &self,
        progress_stream: ProgressStream<S>,
        cluster_progress: InMemProgress,
        summary: StoreSummary,
    ) -> StoreChangeStream<S>;
}

impl<S> GarbageCollect<S> for StoreSummaryStream<S>
where
    S: Scope<Timestamp = u64>,
{
    fn garbage_collect(
        &self,
        progress_stream: ProgressStream<S>,
        cluster_progress: InMemProgress,
        summary: StoreSummary,
    ) -> StoreChangeStream<S> {
        // This is effectively "binary aggregate with epoch" but Timely
        // doesn't give us that.
        self.binary_frontier(
            &progress_stream,
            Exchange::new(|KChange(StoreKey(_epoch, flow_key), _change)| flow_key.route()),
            Pipeline,
            "garbage_collect",
            move |init_cap, _info| {
                let mut summary_inbuf = InBuffer::new();
                let mut progress_inbuf = InBuffer::new();
                let mut ncater = EagerNotificator::new(vec![init_cap], (summary, cluster_progress));

                move |summary_input, progress_input, output| {
                    summary_input.for_each(|cap, incoming| {
                        let epoch = cap.time();
                        summary_inbuf.extend(*epoch, incoming);
                        ncater.notify_at(*epoch);
                    });

                    progress_input.for_each(|cap, incoming| {
                        let epoch = cap.time();
                        progress_inbuf.extend(*epoch, incoming);
                        ncater.notify_at(*epoch);
                    });

                    ncater.for_each(
                        &[
                            summary_input.frontier().clone(),
                            progress_input.frontier().clone(),
                        ],
                        |caps, (summary, cluster_progress)| {
                            let cap = &caps[0];
                            let epoch = cap.time();

                            if let Some(store_changes) = summary_inbuf.remove(epoch) {
                                summary.write_many(store_changes);
                            }
                            if let Some(progress_changes) = progress_inbuf.remove(epoch) {
                                cluster_progress.write_many(progress_changes);
                            }
                        },
                        |caps, (summary, cluster_progress)| {
                            let cap = &caps[0];

                            let ResumeFrom(_ex, resume_epoch) = cluster_progress.resume_from();
                            let mut session = output.session(&cap);
                            session.give_iterator(
                                summary
                                    .drain_garbage(&resume_epoch)
                                    .into_iter()
                                    .map(|key| KChange(key, Change::Discard)),
                            );
                        },
                        |(_summary, _cluster_progress)| {},
                    );
                }
            },
        )
    }
}
