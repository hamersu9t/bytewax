//! Timely operators to implement recovery.
//!
//! They do not implement user-facing dataflow logic.
//!
//! To implement a _recoverable_ operator, see
//! [`crate::operators::stateful_unary`].

use std::collections::hash_map::DefaultHasher;
use std::collections::HashMap;

use std::hash::Hash;
use std::hash::Hasher;
use timely::dataflow::channels::pact::Exchange;
use timely::dataflow::channels::pact::Pipeline;
use timely::dataflow::operators::flow_controlled::iterator_source;
use timely::dataflow::operators::flow_controlled::IteratorSourceInput;
use timely::dataflow::operators::*;
use timely::dataflow::ProbeHandle;
use timely::dataflow::Scope;
use timely::dataflow::ScopeParent;
use timely::dataflow::Stream;
use timely::order::TotalOrder;
use timely::progress::Timestamp;
use timely::Data;
use timely::ExchangeData;

use super::model::*;
use super::store::in_mem::*;

/// All stateful operators take this keyed item as input and emit this
/// as output.
///
/// We need a routeable key to ensure that all items go to the same
/// worker to see the same state.
pub(crate) type StatefulStream<S, V> = Stream<S, (StateKey, V)>;

/// Timely's name for a dataflow stream where you only care about the
/// progress messages.
pub(crate) type ClockStream<S> = Stream<S, Tick>;

/// A stream of changes to some store that can be applied by writing
/// or generated by reading.
type KChangeStream<S, K, V> = Stream<S, KChange<K, V>>;

/// An ordered stream of all changes happening in all stateful operators in a
/// dataflow.
pub(crate) type FlowChangeStream<S> = Stream<S, FlowChange>;

/// An ordered stream of changes to the state store.
pub(crate) type StoreChangeStream<S> = Stream<S, StoreChange<<S as ScopeParent>::Timestamp>>;

/// An ordered stream of changes to the summary the GC operator needs.
pub(crate) type StoreSummaryStream<S> =
    Stream<S, StoreChangeSummary<<S as ScopeParent>::Timestamp>>;

/// A stream of changes to a progress store.
pub(crate) type ProgressStream<S> = Stream<S, ProgressChange<<S as ScopeParent>::Timestamp>>;

impl<K, V> KChange<K, V>
where
    K: Hash,
{
    fn route_by_key() -> impl Fn(&Self) -> u64 {
        |Self(key, _value)| {
            let mut hasher = DefaultHasher::new();
            key.hash(&mut hasher);
            hasher.finish()
        }
    }
}

impl<T, V> KChange<StoreKey<T>, V> {
    fn route_by_flow_key() -> impl Fn(&Self) -> u64 {
        |Self(StoreKey(_epoch, flow_key), _value)| {
            let mut hasher = DefaultHasher::new();
            flow_key.hash(&mut hasher);
            hasher.finish()
        }
    }
}

impl WorkerIndex {
    /// Tell Timely to route to this worker.
    // My read of
    // https://github.com/TimelyDataflow/timely-dataflow/blob/v0.12.0/timely/src/dataflow/channels/pushers/exchange.rs#L61-L90
    // says that if you return the worker index, it'll be
    // routed to that worker.
    pub(crate) fn route(&self) -> u64 {
        self.0 as u64
    }
}

impl StateKey {
    /// Hash this key for Timely.
    ///
    /// Timely uses the result here to decide which worker to send
    /// this data.
    pub(crate) fn route(&self) -> u64 {
        match self {
            Self::Hash(key) => {
                let mut hasher = DefaultHasher::new();
                key.hash(&mut hasher);
                hasher.finish()
            }
            Self::Worker(index) => index.route(),
        }
    }
}

pub(crate) trait Backup<S>
where
    S: Scope,
{
    /// Turn dataflow state changes into an epoch-stamped changelog of
    /// writes to the state store.
    ///
    /// The reverse of this is [`Recover::recover`].
    fn backup(&self) -> StoreChangeStream<S>;
}

impl<S> Backup<S> for FlowChangeStream<S>
where
    S: Scope,
{
    fn backup(&self) -> StoreChangeStream<S> {
        let mut tmp_incoming = Vec::new();

        // This is effectively "map with epoch", but Timely doesn't
        // give us that.
        self.unary(Pipeline, "backup", |_init_cap, _info| {
            move |input, output| {
                input.for_each(|cap, incoming| {
                    let epoch = cap.time();

                    assert!(tmp_incoming.is_empty());
                    incoming.swap(&mut tmp_incoming);

                    let mut session = output.session(&cap);
                    session.give_iterator(tmp_incoming.drain(..).map(
                        |KChange(flow_key, step_change)| {
                            KChange(
                                StoreKey(epoch.clone(), flow_key),
                                Change::Upsert(step_change),
                            )
                        },
                    ));
                })
            }
        })
    }
}

pub(crate) trait Progress<S, D>
where
    S: Scope,
    D: Data,
{
    /// Write out all epochs from upstream that are completed.
    ///
    /// The write that an epoch is complete happens within that epoch.
    ///
    /// Doesn't emit a progress update on dataflow termination to
    /// allow for continuation in another execution.
    fn progress(&self, worker_key: WorkerKey) -> ProgressStream<S>;
}

impl<S, D> Progress<S, D> for Stream<S, D>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    D: Data,
{
    fn progress(&self, worker_key: WorkerKey) -> ProgressStream<S> {
        let mut tmp_incoming = Vec::new();

        // Sort of "emit at end of epoch" but Timely doesn't give us
        // that.
        self.unary_notify(Pipeline, "progress", None, move |input, output, ncater| {
            input.for_each(|cap, incoming| {
                assert!(tmp_incoming.is_empty());
                incoming.swap(&mut tmp_incoming);
                // We have to drain the incoming data, but we just
                // care about the epoch so drop it.
                tmp_incoming.clear();

                ncater.notify_at(cap.retain());
            });

            ncater.for_each(|cap, _count, _ncater| {
                let epoch = BorderEpoch(cap.time().clone());
                let kchange = KChange(worker_key.clone(), Change::Upsert(epoch));
                output.session(&cap).give(kchange);
            });
        })
    }
}

pub(crate) trait Write<S, K, V, W>
where
    S: Scope,
    K: ExchangeData + Hash,
    V: ExchangeData,
    W: KWriter<K, V> + 'static,
{
    /// Write a stream of keyed changes to a matching store in order.
    ///
    /// [`read`] should return an equivalent version of the written
    /// stream.
    fn write(&self, writer: W) -> ClockStream<S>;
}

impl<S, K, V, W> Write<S, K, V, W> for KChangeStream<S, K, V>
where
    S: Scope,
    K: ExchangeData + Hash,
    V: ExchangeData,
    W: KWriter<K, V> + 'static,
{
    fn write(&self, mut writer: W) -> ClockStream<S> {
        let mut tmp_incoming = Vec::new();
        let mut incoming_buffer = HashMap::new();

        // This is effectively "aggregate" but with `KChange`s not `(K,
        // V)`. It's just slightly longer, but clearer to write it out
        // ourselves.
        self.unary_notify(
            Exchange::new(KChange::route_by_key()),
            "write",
            None,
            move |input, output, ncater| {
                input.for_each(|cap, incoming| {
                    let epoch = cap.time();

                    assert!(tmp_incoming.is_empty());
                    incoming.swap(&mut tmp_incoming);

                    incoming_buffer
                        .entry(epoch.clone())
                        .or_insert_with(Vec::new)
                        .append(&mut tmp_incoming);

                    ncater.notify_at(cap.retain());
                });

                // Use notificator so writes are in epoch order per
                // key.
                ncater.for_each(|cap, _count, _ncater| {
                    let epoch = cap.time();

                    if let Some(incoming) = incoming_buffer.remove(epoch) {
                        writer.write_many(incoming);

                        let mut session = output.session(&cap);
                        session.give(());
                    }
                });
            },
        )
    }
}

/// Read a stream of keyed changes from a matching store in order.
///
/// [`Write::write`] should be used to write that stream.
pub(crate) fn read<S, K, V, R>(
    scope: &S,
    mut reader: R,
    probe: &ProbeHandle<S::Timestamp>,
) -> KChangeStream<S, K, V>
where
    S: Scope,
    S::Timestamp: TotalOrder,
    K: Data,
    V: Data,
    R: KReader<K, V> + 'static,
{
    iterator_source(
        scope,
        "read",
        move |last_cap| {
            // Assumes that changes are read in order they are
            // written.
            reader.read_many().map(|changes| IteratorSourceInput {
                lower_bound: S::Timestamp::minimum(),
                // An iterator of (timestamp, iterator of
                // items). Nested [`IntoIterator`]s.
                data: Some((S::Timestamp::minimum(), changes)),
                target: last_cap.clone(),
            })
        },
        probe.clone(),
    )
}

pub(crate) trait Recover<S>
where
    S: Scope,
{
    /// Collapse a stream of state store changes into a stream of
    /// dataflow state changes.
    ///
    /// This effectively reverses [`Backup::backup`] by discarding any
    /// state information that's not from the latest epoch.
    ///
    /// If you don't want to collapse to the latest state, filter out
    /// the state store changes by epoch.
    fn recover(&self) -> FlowChangeStream<S>;
}

impl<S> Recover<S> for StoreChangeStream<S>
where
    S: Scope,
{
    fn recover(&self) -> FlowChangeStream<S> {
        let mut tmp_incoming = Vec::new();
        let mut state = InMemStore::new();

        self.unary_notify(
            Exchange::new(KChange::route_by_flow_key()),
            "recover",
            None,
            move |input, output, ncater| {
                input.for_each(|cap, incoming| {
                    assert!(tmp_incoming.is_empty());
                    incoming.swap(&mut tmp_incoming);

                    state.write_many(tmp_incoming.drain(..).collect());
                    // Remove all but the newest changes so we don't
                    // have to have the whole recovery store in mem.
                    state.filter_last();

                    ncater.notify_at(cap.retain());
                });

                ncater.for_each(|cap, _count, _ncater| {
                    let mut session = output.session(&cap);
                    session.give_iterator(state.drain_flatten().into_iter());
                });
            },
        )
    }
}

pub(crate) trait Summary<S>
where
    S: Scope,
{
    /// Drop the data contained within state store changes.
    fn summary(&self) -> StoreSummaryStream<S>;
}

impl<S> Summary<S> for StoreChangeStream<S>
where
    S: Scope,
{
    fn summary(&self) -> StoreSummaryStream<S> {
        self.map(|KChange(key, change)| KChange(key, change.map(|c| c.typ())))
    }
}

pub(crate) trait GarbageCollect<S>
where
    S: Scope,
{
    /// Generate deletes for un-needed state store data.
    ///
    /// This looks at the stream of worker progress events, maintains
    /// a snapshot of the current cluster frontier, and emits delete
    /// changes when historical data is no longer necessary for
    /// recovery.
    ///
    /// Note, GCing a write to the state store is not the same as that
    /// state being discarded by user logic: the fact that state was
    /// discarded is itself written to the state change log; GC is
    /// when we no longer need that part of the change log because the
    /// work for that epoch is finalized.
    ///
    /// This takes a summary of writes to the state store in order to
    /// not actually need to hold the state in memory.
    fn garbage_collect(
        &self,
        progress_stream: ProgressStream<S>,
        summary: StoreSummary<S::Timestamp>,
    ) -> StoreChangeStream<S>;
}

impl<S> GarbageCollect<S> for StoreSummaryStream<S>
where
    S: Scope<Timestamp = u64>,
{
    fn garbage_collect(
        &self,
        progress_stream: ProgressStream<S>,
        mut summary: StoreSummary<S::Timestamp>,
    ) -> StoreChangeStream<S> {
        let mut tmp_summary = Vec::new();
        let mut tmp_progress = Vec::new();

        let mut store_buffer = HashMap::new();
        let mut progress_buffer = HashMap::new();

        let mut cluster_progress = InMemProgress::new();

        // This is effectively "binary aggregate with epoch" but Timely
        // doesn't give us that.
        self.binary_notify(
            &progress_stream,
            Exchange::new(KChange::route_by_flow_key()),
            Pipeline,
            "garbage_collect",
            None,
            move |summary_input, progress_input, output, ncater| {
                summary_input.for_each(|cap, incoming| {
                    let epoch = cap.time();

                    assert!(tmp_summary.is_empty());
                    incoming.swap(&mut tmp_summary);

                    store_buffer
                        .entry(epoch.clone())
                        .or_insert_with(Vec::new)
                        .append(&mut tmp_summary);

                    ncater.notify_at(cap.retain());
                });

                progress_input.for_each(|cap, incoming| {
                    let epoch = cap.time();

                    assert!(tmp_progress.is_empty());
                    incoming.swap(&mut tmp_progress);

                    progress_buffer
                        .entry(epoch.clone())
                        .or_insert_with(Vec::new)
                        .append(&mut tmp_progress);

                    ncater.notify_at(cap.retain());
                });

                ncater.for_each(|cap, _count, _ncater| {
                    let epoch = cap.time();

                    if let Some(store_changes) = store_buffer.remove(epoch) {
                        summary.write_many(store_changes);
                    }
                    if let Some(progress_changes) = progress_buffer.remove(epoch) {
                        cluster_progress.write_many(progress_changes);
                    }

                    let mut session = output.session(&cap);
                    session.give_iterator(
                        summary
                            .drain_garbage(&cluster_progress.resume_epoch())
                            .into_iter()
                            .map(|key| KChange(key, Change::Discard)),
                    );
                });
            },
        )
    }
}
