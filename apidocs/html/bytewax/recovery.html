<main class="api__content">
<article class="api__article" id="content">
<header class="api__article-header">
<h1 class="api__article-title">Module <strong>bytewax.recovery</strong></h1>
</header>
<section class="api__article-intro" id="section-intro">
<p>Recovering from failures.</p>
<p>Bytewax allows you to <strong>recover</strong> a stateful dataflow; it will let you
resume processing and output due to a failure without re-processing
all initial data to re-calculate all internal state.</p>
<p>It does this by storing state and progress information for a single
dataflow instance in a <strong>recovery store</strong> backed by a durable state
storage system of your choosing, e.g. SQLite or Kafka. See the
subclasses of <code><a title="bytewax.recovery.RecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.RecoveryConfig">RecoveryConfig</a></code> in this module for the supported
datastores, the specifics of how each is utilized, and tradeoffs.</p>
<h2 id="preparation">Preparation</h2>
<ol>
<li>
<p>Create a <strong>recovery config</strong> for describing how to connect to the
recovery store of your choosing.</p>
</li>
<li>
<p>Pass that recovery config as the <code>recovery_config</code> argument to the
entry point running your dataflow (e.g. <code>bytewax.cluster_main()</code>).</p>
</li>
</ol>
<h2 id="execution">Execution</h2>
<p>Make sure your worker processes have access to the recovery store.</p>
<p>Then, run your dataflow! It will start backing up recovery data
automatically.</p>
<h2 id="recovering">Recovering</h2>
<p>If the dataflow fails, first you must fix whatever underlying fault
caused the issue. That might mean deploying new code which fixes a bug
or resolving an issue with a connected system.</p>
<p>Once that is done, re-run the dataflow using the <em>same recovery
config</em> and thus re-connect to the <em>same recovery store</em>. Bytewax will
automatically read the progress of the previous dataflow execution and
determine the most recent point that processing can resume at. Output
should resume from that point.</p>
<p>If you want to fully restart a dataflow and ignore previous state,
delete the data in the recovery store using whatever operational tools
you have for that storage type.</p>
<h2 id="caveats">Caveats</h2>
<p>Recovery data for multiple dataflows <em>must not</em> be mixed together. See
the docs for each <code><a title="bytewax.recovery.RecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.RecoveryConfig">RecoveryConfig</a></code> subclass for what this means
depending on the recovery store. E.g. when using a
<code><a title="bytewax.recovery.KafkaRecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.KafkaRecoveryConfig">KafkaRecoveryConfig</a></code>, each dataflow must have a distinct topic
prefix.</p>
<p>See comments on input configuration types in <code><a title="bytewax.inputs" href="/apidocs/bytewax.inputs">bytewax.inputs</a></code> for any
limitations each might have regarding recovery.</p>
<p>It is possible that your output systems will see duplicate data around
the resume point; design your systems to support at-least-once
processing.</p>
<p>Currently it is not possible to recover a dataflow with a different
number of workers than when it failed.</p>
<p>The epoch is the time unit of recovery: dataflows will only resume on
epoch boundaries, with the <strong>resume epoch</strong> being where the dataflow
witll resume. Bytewax defaults to a new epoch every 10 seconds. See
<code>bytewax.execution.EpochConfig</code> for more info on epochs.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">&#34;&#34;&#34;Recovering from failures.

Bytewax allows you to **recover** a stateful dataflow; it will let you
resume processing and output due to a failure without re-processing
all initial data to re-calculate all internal state.

It does this by storing state and progress information for a single
dataflow instance in a **recovery store** backed by a durable state
storage system of your choosing, e.g. SQLite or Kafka. See the
subclasses of `RecoveryConfig` in this module for the supported
datastores, the specifics of how each is utilized, and tradeoffs.

Preparation
-----------

1. Create a **recovery config** for describing how to connect to the
recovery store of your choosing.

2. Pass that recovery config as the `recovery_config` argument to the
entry point running your dataflow (e.g. `bytewax.cluster_main()`).

Execution
---------

Make sure your worker processes have access to the recovery store.

Then, run your dataflow! It will start backing up recovery data
automatically.

Recovering
----------

If the dataflow fails, first you must fix whatever underlying fault
caused the issue. That might mean deploying new code which fixes a bug
or resolving an issue with a connected system.

Once that is done, re-run the dataflow using the _same recovery
config_ and thus re-connect to the _same recovery store_. Bytewax will
automatically read the progress of the previous dataflow execution and
determine the most recent point that processing can resume at. Output
should resume from that point.

If you want to fully restart a dataflow and ignore previous state,
delete the data in the recovery store using whatever operational tools
you have for that storage type.

Caveats
-------

Recovery data for multiple dataflows _must not_ be mixed together. See
the docs for each `RecoveryConfig` subclass for what this means
depending on the recovery store. E.g. when using a
`KafkaRecoveryConfig`, each dataflow must have a distinct topic
prefix.

See comments on input configuration types in `bytewax.inputs` for any
limitations each might have regarding recovery.

It is possible that your output systems will see duplicate data around
the resume point; design your systems to support at-least-once
processing.

Currently it is not possible to recover a dataflow with a different
number of workers than when it failed.

The epoch is the time unit of recovery: dataflows will only resume on
epoch boundaries, with the **resume epoch** being where the dataflow
witll resume. Bytewax defaults to a new epoch every 10 seconds. See
`bytewax.execution.EpochConfig` for more info on epochs.

&#34;&#34;&#34;

from .bytewax import (  # noqa: F401
    KafkaRecoveryConfig,
    RecoveryConfig,
    SqliteRecoveryConfig,
)

__all__ = [
    &#34;KafkaRecoveryConfig&#34;,
    &#34;RecoveryConfig&#34;,
    &#34;SqliteRecoveryConfig&#34;,
]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="api__article-subtitle" id="header-classes">Classes</h2>
<dl>
<dt id="bytewax.recovery.KafkaRecoveryConfig"><code class="language-python flex name class">
<span>class <span class="ident">KafkaRecoveryConfig</span></span>
<span>(</span><span>brokers, topic_prefix)</span>
</code></dt>
<dd>
<div class="desc"><p>Use <a href="https://kafka.apache.org/">Kafka</a> to store recovery data.</p>
<p>Uses a "progress" topic and a "state" topic with a number of
partitions equal to the number of workers. Will take advantage of
log compaction so that topic size is proportional to state size,
not epoch count.</p>
<p>Use a distinct topic prefix per dataflow so recovery data is not
mixed.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; from bytewax.testing import run_main, TestingInput
&gt;&gt;&gt; from bytewax.connectors.stdio import StdOutput
&gt;&gt;&gt; from bytewax.dataflow import Dataflow
&gt;&gt;&gt; flow = Dataflow()
&gt;&gt;&gt; flow.input(&quot;inp&quot;, TestingInput(range(3)))
&gt;&gt;&gt; flow.output(&quot;out&quot;, StdOutput())
&gt;&gt;&gt; recovery_config = KafkaRecoveryConfig(
...     [&quot;localhost:9092&quot;],
...     &quot;sample-dataflow&quot;,
... )
&gt;&gt;&gt; run_main(
...     flow,
...     recovery_config=recovery_config,
... )  # doctest: +ELLIPSIS
</code></pre>
<p>If there's no previous recovery data, topics will automatically be
created with the correct number of partitions and log compaction
enabled</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>brokers</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of <code>host:port</code> strings of Kafka
brokers.</dd>
<dt><strong><code>topic_prefix</code></strong> :&ensp;<code>str</code></dt>
<dd>Prefix used for naming topics. Must be
distinct per-dataflow. Two topics will be created using
this prefix <code>"topic_prefix-progress"</code> and
<code>"topic_prefix-state"</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Config object. Pass this as the <code>recovery_config</code> argument to
your execution entry point.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.recovery.RecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.RecoveryConfig">RecoveryConfig</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="bytewax.recovery.KafkaRecoveryConfig.brokers"><code class="language-python name">var <span class="ident">brokers</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
<dt id="bytewax.recovery.KafkaRecoveryConfig.topic_prefix"><code class="language-python name">var <span class="ident">topic_prefix</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
</dd>
<dt id="bytewax.recovery.RecoveryConfig"><code class="language-python flex name class">
<span>class <span class="ident">RecoveryConfig</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for a recovery config.</p>
<p>This describes how each worker in a dataflow cluster should store
its recovery data.</p>
<p>Use a specific subclass of this that matches the kind of storage
system you are going to use. See the subclasses in this module.</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="bytewax.recovery.KafkaRecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.KafkaRecoveryConfig">KafkaRecoveryConfig</a></li>
<li><a title="bytewax.recovery.SqliteRecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.SqliteRecoveryConfig">SqliteRecoveryConfig</a></li>
</ul>
</dd>
<dt id="bytewax.recovery.SqliteRecoveryConfig"><code class="language-python flex name class">
<span>class <span class="ident">SqliteRecoveryConfig</span></span>
<span>(</span><span>db_dir)</span>
</code></dt>
<dd>
<div class="desc"><p>Use <a href="https://sqlite.org/index.html">SQLite</a> to store recovery
data.</p>
<p>Creates a SQLite DB per-worker in a given directory. Multiple DBs
are used to allow workers to write without contention.</p>
<p>Use a distinct directory per dataflow so recovery data is not
mixed.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; from bytewax.testing import run_main, TestingInput
&gt;&gt;&gt; from bytewax.connectors.stdio import StdOutput
&gt;&gt;&gt; from bytewax.dataflow import Dataflow
&gt;&gt;&gt; flow = Dataflow()
&gt;&gt;&gt; flow.input(&quot;inp&quot;, TestingInput(range(3)))
&gt;&gt;&gt; flow.output(&quot;out&quot;, StdOutput())
&gt;&gt;&gt; recovery_config = SqliteRecoveryConfig(&quot;.&quot;)
&gt;&gt;&gt; run_main(
...     flow,
...     recovery_config=recovery_config,
... )  # doctest: +ELLIPSIS
</code></pre>
<p>DB files and tables will automatically be created if there's no
previous recovery data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>db_dir</code></strong> :&ensp;<code>Path</code></dt>
<dd>Existing directory to store per-worker DBs
in. Must be distinct per-dataflow. DB files will have
names like <code>"worker0.sqlite3"</code>. You can use <code>"."</code> for the
current directory.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Config object. Pass this as the <code>recovery_config</code> argument to
your execution entry point.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.recovery.RecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.RecoveryConfig">RecoveryConfig</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="bytewax.recovery.SqliteRecoveryConfig.db_dir"><code class="language-python name">var <span class="ident">db_dir</span></code></dt>
<dd>
<div class="desc"><p>Return an attribute of instance, which is of type owner.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
<footer class="api__footer" id="footer">
<p class="api__footer-copyright">
Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.
</p>
</footer>
</article>
<nav class="api__sidebar" id="sidebar">
<ul class="api__sidebar-nav" id="index">
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title">Super-module</h3>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item">
<a title="bytewax" href="/apidocs/">bytewax</a>
</li>
</ul>
</li>
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title"><a href="#header-classes">Classes</a></h3>
<ul class="api__sidebar-nav-classes">
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.recovery.KafkaRecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.KafkaRecoveryConfig">KafkaRecoveryConfig</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.recovery.KafkaRecoveryConfig.brokers" href="/apidocs/bytewax.recovery#bytewax.recovery.KafkaRecoveryConfig.brokers">brokers</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.recovery.KafkaRecoveryConfig.topic_prefix" href="/apidocs/bytewax.recovery#bytewax.recovery.KafkaRecoveryConfig.topic_prefix">topic_prefix</a></li>
</ul>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.recovery.RecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.RecoveryConfig">RecoveryConfig</a></h4>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.recovery.SqliteRecoveryConfig" href="/apidocs/bytewax.recovery#bytewax.recovery.SqliteRecoveryConfig">SqliteRecoveryConfig</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.recovery.SqliteRecoveryConfig.db_dir" href="/apidocs/bytewax.recovery#bytewax.recovery.SqliteRecoveryConfig.db_dir">db_dir</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>