<main class="api__content">
<article class="api__article" id="content">
<header class="api__article-header">
<h1 class="api__article-title">Module <strong>bytewax.connectors.kafka</strong></h1>
</header>
<section class="api__article-intro" id="section-intro">
<p>Connectors for <a href="https://kafka.apache.org">Kafka</a>.</p>
<p>Importing this module requires the
<a href="https://github.com/confluentinc/confluent-kafka-python"><code>confluent-kafka</code></a>
package to be installed.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">&#34;&#34;&#34;Connectors for [Kafka](https://kafka.apache.org).

Importing this module requires the
[`confluent-kafka`](https://github.com/confluentinc/confluent-kafka-python)
package to be installed.

&#34;&#34;&#34;
from typing import Dict, Iterable

from confluent_kafka import (
    Consumer,
    KafkaError,
    OFFSET_BEGINNING,
    Producer,
    TopicPartition,
)
from confluent_kafka.admin import AdminClient

from bytewax.inputs import PartitionedInput, StatefulSource
from bytewax.outputs import DynamicOutput, StatelessSink


__all__ = [
    &#34;KafkaInput&#34;,
    &#34;KafkaOutput&#34;,
]


def _list_parts(client, topics):
    for topic in topics:
        # List topics one-by-one so if auto-create is turned on,
        # we respect that.
        cluster_metadata = client.list_topics(topic)
        topic_metadata = cluster_metadata.topics[topic]
        if topic_metadata.error is not None:
            raise RuntimeError(
                f&#34;error listing partitions for Kafka topic `{topic!r}`: &#34;
                f&#34;{topic_metadata.error.str()}&#34;
            )
        part_idxs = topic_metadata.partitions.keys()
        for i in part_idxs:
            yield f&#34;{i}-{topic}&#34;


class _KafkaSource(StatefulSource):
    def __init__(self, consumer, topic, part_idx, starting_offset, resume_state):
        self._offset = resume_state or starting_offset
        # Assign does not activate consumer grouping.
        consumer.assign([TopicPartition(topic, part_idx, self._offset)])
        self._consumer = consumer
        self._topic = topic

    def next(self):
        msg = self._consumer.poll(0.001)  # seconds
        if msg is None:
            return
        elif msg.error() is not None:
            if msg.error().code() == KafkaError._PARTITION_EOF:
                raise StopIteration()
            else:
                raise RuntimeError(
                    f&#34;error consuming from Kafka topic `{self.topic!r}`: {msg.error()}&#34;
                )
        else:
            item = (msg.key(), msg.value())
            # Resume reading from the next message, not this one.
            self._offset = msg.offset() + 1
            return item

    def snapshot(self):
        return self._offset

    def close(self):
        self._consumer.close()


class KafkaInput(PartitionedInput):
    &#34;&#34;&#34;Use [Kafka](https://kafka.apache.org) topics as an input
    source.

    Kafka messages are emitted into the dataflow as two-tuples of
    `(key_bytes, value_bytes)`.

    Partitions are the unit of parallelism.

    Can support exactly-once processing.

    Args:

        brokers: List of `host:port` strings of Kafka brokers.

        topics: List of topics to consume from.

        tail: Whether to wait for new data on this topic when the end
            is initially reached.

        starting_offset: Can be either
            `confluent_kafka.OFFSET_BEGINNING` or
            `confluent_kafka.OFFSET_END`. Defaults to beginning of
            topic.

        add_config: Any additional configuration properties. See [the
            `rdkafka`
            documentation](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md)
            for options.

    &#34;&#34;&#34;

    def __init__(
        self,
        brokers: Iterable[str],
        topics: Iterable[str],
        tail: bool = True,
        starting_offset: int = OFFSET_BEGINNING,
        add_config: Dict[str, str] = None,
    ):
        add_config = add_config or {}

        if isinstance(brokers, str):
            raise TypeError(&#34;brokers must be an iterable and not a string&#34;)
        self._brokers = brokers
        if isinstance(topics, str):
            raise TypeError(&#34;topics must be an iterable and not a string&#34;)
        self._topics = topics
        self._tail = tail
        self._starting_offset = starting_offset
        self._add_config = add_config

    def list_parts(self):
        config = {
            &#34;bootstrap.servers&#34;: &#34;,&#34;.join(self._brokers),
        }
        config.update(self._add_config)
        client = AdminClient(config)

        return set(_list_parts(client, self._topics))

    def build_part(self, for_part, resume_state):
        part_idx, topic = for_part.split(&#34;-&#34;, 1)
        part_idx = int(part_idx)
        # TODO: Warn and then return None. This might be an indication
        # of dataflow continuation with a new topic (to enable
        # re-partitioning), which is fine.
        assert topic in self._topics, &#34;Can&#39;t resume from different set of Kafka topics&#34;

        config = {
            # We&#39;ll manage our own &#34;consumer group&#34; via recovery
            # system.
            &#34;group.id&#34;: &#34;BYTEWAX_IGNORED&#34;,
            &#34;enable.auto.commit&#34;: &#34;false&#34;,
            &#34;bootstrap.servers&#34;: &#34;,&#34;.join(self._brokers),
            &#34;enable.partition.eof&#34;: str(not self._tail),
        }
        config.update(self._add_config)
        consumer = Consumer(config)
        return _KafkaSource(
            consumer, topic, part_idx, self._starting_offset, resume_state
        )


class _KafkaSink(StatelessSink):
    def __init__(self, producer, topic):
        self._producer = producer
        self._topic = topic

    def write(self, key_value):
        key, value = key_value
        self._producer.produce(self._topic, value, key)
        self._producer.flush()

    def close(self):
        self._producer.flush()


class KafkaOutput(DynamicOutput):
    &#34;&#34;&#34;Use a single [Kafka](https://kafka.apache.org) topic as an
    output sink.

    Items consumed from the dataflow must look like two-tuples of
    `(key_bytes, value_bytes)`. Default partition routing is used.

    Workers are the unit of parallelism.

    Can support at-least-once processing. Messages from the resume
    epoch will be duplicated right after resume.

    Args:

        brokers: List of `host:port` strings of Kafka brokers.

        topic: Topic to produce to.

        add_config: Any additional configuration properties. See [the
            `rdkafka`
            documentation](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md)
            for options.

    &#34;&#34;&#34;

    def __init__(
        self,
        brokers: Iterable[str],
        topic: str,
        add_config: Dict[str, str] = None,
    ):
        add_config = add_config or {}

        self._brokers = brokers
        self._topic = topic
        self._add_config = add_config

    def build(self, worker_index, worker_count):
        config = {
            &#34;bootstrap.servers&#34;: &#34;,&#34;.join(self._brokers),
        }
        config.update(self._add_config)
        producer = Producer(config)
        return _KafkaSink(producer, self._topic)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="api__article-subtitle" id="header-classes">Classes</h2>
<dl>
<dt id="bytewax.connectors.kafka.KafkaInput"><code class="language-python flex name class">
<span>class <span class="ident">KafkaInput</span></span>
<span>(</span><span>brokers: Iterable[str], topics: Iterable[str], tail: bool = True, starting_offset: int = -2, add_config: Dict[str, str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Use <a href="https://kafka.apache.org">Kafka</a> topics as an input
source.</p>
<p>Kafka messages are emitted into the dataflow as two-tuples of
<code>(key_bytes, value_bytes)</code>.</p>
<p>Partitions are the unit of parallelism.</p>
<p>Can support exactly-once processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>brokers</code></strong></dt>
<dd>List of <code>host:port</code> strings of Kafka brokers.</dd>
<dt><strong><code>topics</code></strong></dt>
<dd>List of topics to consume from.</dd>
<dt><strong><code>tail</code></strong></dt>
<dd>Whether to wait for new data on this topic when the end
is initially reached.</dd>
<dt><strong><code>starting_offset</code></strong></dt>
<dd>Can be either
<code>confluent_kafka.OFFSET_BEGINNING</code> or
<code>confluent_kafka.OFFSET_END</code>. Defaults to beginning of
topic.</dd>
<dt><strong><code>add_config</code></strong></dt>
<dd>Any additional configuration properties. See <a href="https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md">the
<code>rdkafka</code>
documentation</a>
for options.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class KafkaInput(PartitionedInput):
    &#34;&#34;&#34;Use [Kafka](https://kafka.apache.org) topics as an input
    source.

    Kafka messages are emitted into the dataflow as two-tuples of
    `(key_bytes, value_bytes)`.

    Partitions are the unit of parallelism.

    Can support exactly-once processing.

    Args:

        brokers: List of `host:port` strings of Kafka brokers.

        topics: List of topics to consume from.

        tail: Whether to wait for new data on this topic when the end
            is initially reached.

        starting_offset: Can be either
            `confluent_kafka.OFFSET_BEGINNING` or
            `confluent_kafka.OFFSET_END`. Defaults to beginning of
            topic.

        add_config: Any additional configuration properties. See [the
            `rdkafka`
            documentation](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md)
            for options.

    &#34;&#34;&#34;

    def __init__(
        self,
        brokers: Iterable[str],
        topics: Iterable[str],
        tail: bool = True,
        starting_offset: int = OFFSET_BEGINNING,
        add_config: Dict[str, str] = None,
    ):
        add_config = add_config or {}

        if isinstance(brokers, str):
            raise TypeError(&#34;brokers must be an iterable and not a string&#34;)
        self._brokers = brokers
        if isinstance(topics, str):
            raise TypeError(&#34;topics must be an iterable and not a string&#34;)
        self._topics = topics
        self._tail = tail
        self._starting_offset = starting_offset
        self._add_config = add_config

    def list_parts(self):
        config = {
            &#34;bootstrap.servers&#34;: &#34;,&#34;.join(self._brokers),
        }
        config.update(self._add_config)
        client = AdminClient(config)

        return set(_list_parts(client, self._topics))

    def build_part(self, for_part, resume_state):
        part_idx, topic = for_part.split(&#34;-&#34;, 1)
        part_idx = int(part_idx)
        # TODO: Warn and then return None. This might be an indication
        # of dataflow continuation with a new topic (to enable
        # re-partitioning), which is fine.
        assert topic in self._topics, &#34;Can&#39;t resume from different set of Kafka topics&#34;

        config = {
            # We&#39;ll manage our own &#34;consumer group&#34; via recovery
            # system.
            &#34;group.id&#34;: &#34;BYTEWAX_IGNORED&#34;,
            &#34;enable.auto.commit&#34;: &#34;false&#34;,
            &#34;bootstrap.servers&#34;: &#34;,&#34;.join(self._brokers),
            &#34;enable.partition.eof&#34;: str(not self._tail),
        }
        config.update(self._add_config)
        consumer = Consumer(config)
        return _KafkaSource(
            consumer, topic, part_idx, self._starting_offset, resume_state
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.inputs.PartitionedInput" href="/apidocs/bytewax.inputs#bytewax.inputs.PartitionedInput">PartitionedInput</a></li>
<li><a title="bytewax.inputs.Input" href="/apidocs/bytewax.inputs#bytewax.inputs.Input">Input</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code class="language-python"><b><a title="bytewax.inputs.PartitionedInput" href="/apidocs/bytewax.inputs#bytewax.inputs.PartitionedInput">PartitionedInput</a></b></code>:
<ul class="hlist">
<li><code class="language-python"><a title="bytewax.inputs.PartitionedInput.build_part" href="/apidocs/bytewax.inputs#bytewax.inputs.PartitionedInput.build_part">build_part</a></code></li>
<li><code class="language-python"><a title="bytewax.inputs.PartitionedInput.list_parts" href="/apidocs/bytewax.inputs#bytewax.inputs.PartitionedInput.list_parts">list_parts</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="bytewax.connectors.kafka.KafkaOutput"><code class="language-python flex name class">
<span>class <span class="ident">KafkaOutput</span></span>
<span>(</span><span>brokers: Iterable[str], topic: str, add_config: Dict[str, str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Use a single <a href="https://kafka.apache.org">Kafka</a> topic as an
output sink.</p>
<p>Items consumed from the dataflow must look like two-tuples of
<code>(key_bytes, value_bytes)</code>. Default partition routing is used.</p>
<p>Workers are the unit of parallelism.</p>
<p>Can support at-least-once processing. Messages from the resume
epoch will be duplicated right after resume.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>brokers</code></strong></dt>
<dd>List of <code>host:port</code> strings of Kafka brokers.</dd>
<dt><strong><code>topic</code></strong></dt>
<dd>Topic to produce to.</dd>
<dt><strong><code>add_config</code></strong></dt>
<dd>Any additional configuration properties. See <a href="https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md">the
<code>rdkafka</code>
documentation</a>
for options.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class KafkaOutput(DynamicOutput):
    &#34;&#34;&#34;Use a single [Kafka](https://kafka.apache.org) topic as an
    output sink.

    Items consumed from the dataflow must look like two-tuples of
    `(key_bytes, value_bytes)`. Default partition routing is used.

    Workers are the unit of parallelism.

    Can support at-least-once processing. Messages from the resume
    epoch will be duplicated right after resume.

    Args:

        brokers: List of `host:port` strings of Kafka brokers.

        topic: Topic to produce to.

        add_config: Any additional configuration properties. See [the
            `rdkafka`
            documentation](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md)
            for options.

    &#34;&#34;&#34;

    def __init__(
        self,
        brokers: Iterable[str],
        topic: str,
        add_config: Dict[str, str] = None,
    ):
        add_config = add_config or {}

        self._brokers = brokers
        self._topic = topic
        self._add_config = add_config

    def build(self, worker_index, worker_count):
        config = {
            &#34;bootstrap.servers&#34;: &#34;,&#34;.join(self._brokers),
        }
        config.update(self._add_config)
        producer = Producer(config)
        return _KafkaSink(producer, self._topic)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.outputs.DynamicOutput" href="/apidocs/bytewax.outputs#bytewax.outputs.DynamicOutput">DynamicOutput</a></li>
<li><a title="bytewax.outputs.Output" href="/apidocs/bytewax.outputs#bytewax.outputs.Output">Output</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code class="language-python"><b><a title="bytewax.outputs.DynamicOutput" href="/apidocs/bytewax.outputs#bytewax.outputs.DynamicOutput">DynamicOutput</a></b></code>:
<ul class="hlist">
<li><code class="language-python"><a title="bytewax.outputs.DynamicOutput.build" href="/apidocs/bytewax.outputs#bytewax.outputs.DynamicOutput.build">build</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
<footer class="api__footer" id="footer">
<p class="api__footer-copyright">
Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.
</p>
</footer>
</article>
<nav class="api__sidebar" id="sidebar">
<ul class="api__sidebar-nav" id="index">
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title">Super-module</h3>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item">
<a title="bytewax.connectors" href="/apidocs/bytewax.connectors/index">bytewax.connectors</a>
</li>
</ul>
</li>
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title"><a href="#header-classes">Classes</a></h3>
<ul class="api__sidebar-nav-classes">
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.connectors.kafka.KafkaInput" href="/apidocs/bytewax.connectors/kafka#bytewax.connectors.kafka.KafkaInput">KafkaInput</a></h4>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.connectors.kafka.KafkaOutput" href="/apidocs/bytewax.connectors/kafka#bytewax.connectors.kafka.KafkaOutput">KafkaOutput</a></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>